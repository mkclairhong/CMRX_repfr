# AUTOGENERATED! DO NOT EDIT! File to edit: 04_Repetition_Effects.ipynb (unless otherwise specified).

__all__ = ['encoding_states', 'plot_states', 'encoding_visualizations', 'retrieval_states', 'outcome_probs_at_index',
           'retrieval_visualizations', 'visualize_individuals', 'visualize_aggregate', 'visualize_model',
           'visualize_fit', 'recall_probability_by_lag', 'sim_recall_probability_by_lag']

# Cell

import numpy as np

def encoding_states(model):
    """
    Tracks state of context, and item supports across encoding. Model is also advanced to a state of fully encoded
    memories.

    **Required model attributes**:
    - item_count: specifies number of items encoded into memory
    - context: vector representing an internal contextual state
    - experience: adding a new trace to the memory model
    - activations: function returning item activations given a vector probe
    - outcome_probabilities: function returning item supports given a set of activations

    **Returns** array representations of context and support for retrieval of each item at each increment of item
    encoding. Each has shape model.item_count by model.item_count + 1.
    """

    experiences = np.eye(model.item_count, model.item_count + 1, 1)
    cmr_experiences = np.eye(model.item_count, model.item_count)
    encoding_contexts, encoding_supports = model.context, []

    # track model state across experiences
    for i in range(len(experiences)):
        model.experience(cmr_experiences[i].reshape((1, -1)))

        # track model contexts and item supports
        encoding_contexts = np.vstack((encoding_contexts, model.context))

        activation_cue = lambda model: model.context

        if len(encoding_supports) > 0:
            encoding_supports = np.vstack((encoding_supports, model.outcome_probabilities(activation_cue(model))))
        else:
            encoding_supports = model.outcome_probabilities(activation_cue(model))

    return encoding_contexts, encoding_supports

# Cell

import seaborn as sns
import matplotlib.pyplot as plt

def plot_states(matrix, title, figsize=(15, 15), savefig=False):
    """
    Plots an array of model states as a value-annotated heatmap with an arbitrary title.

    **Arguments**:
    - matrix: an array of model states, ideally with columns representing unique feature indices and rows
        representing unique update indices
    - title: a title for the generated plot, ideally conveying what array values represent at each entry
    - savefig: boolean deciding whether generated figure is saved (True if Yes)
    """

    plt.figure(figsize=figsize)
    sns.heatmap(matrix, annot=True, linewidths=.5)
    plt.title(title)
    plt.xlabel('Feature Index')
    plt.ylabel('Update Index')
    if savefig:
        plt.savefig('figures/{}.jpeg'.format(title).replace(' ', '_').lower(), bbox_inches='tight')
    plt.show()

# Cell

def encoding_visualizations(model, savefig=True):
    """
    Plots encoding contexts, encoding supports as heatmaps.

    **Required model attributes**:
    - item_count: specifies number of items encoded into memory
    - context: vector representing an internal contextual state
    - experience: adding a new trace to the memory model
    - activations: function returning item activations given a vector probe
    - outcome_probabilities: function returning item supports given a set of activations
    - memory: a unitary representation of the current state of memory

    **Also** requires savefig:  boolean deciding if generated figure is saved
    """

    encoding_contexts, encoding_supports = encoding_states(model)
    plot_states(encoding_contexts, 'Encoding Contexts', savefig=savefig)
    plot_states(encoding_supports, 'Supports For Each Item At Each Increment of Encoding', savefig=savefig)

# Cell

import numpy as np

def retrieval_states(model, first_recall_item=None):
    """
    Tracks state of context, and item supports across retrieval. Model is also advanced into a state of
    completed free recall.

    **Required model attributes**:
    - item_count: specifies number of items encoded into memory
    - context: vector representing an internal contextual state
    - experience: adding a new trace to the memory model
    - activations: function returning item activations given a vector probe
    - outcome_probabilities: function returning item supports given a set of activations
    - free_recall: function that freely recalls a given number of items or until recall stops
    - state: indicates whether model is encoding or engaged in recall with a string

    **Also** optionally uses first_recall_item: can specify an item for first recall

    **Returns** array representations of context and support for retrieval of each item at each increment of item
    retrieval. Also returns recall train associated with simulation.
    """

    activation_cue = lambda model: model.context

    # encoding items, presuming model is freshly initialized
    encoding_states(model)
    retrieval_contexts, retrieval_supports = model.context, model.outcome_probabilities(
        activation_cue(model))

    # pre-retrieval distraction
    model.free_recall(0)
    retrieval_contexts = np.vstack((retrieval_contexts, model.context))
    retrieval_supports = np.vstack((retrieval_supports, model.outcome_probabilities(
        activation_cue(model))))

    # optional forced first item recall
    if first_recall_item is not None:
        model.force_recall(first_recall_item)
        retrieval_contexts = np.vstack((retrieval_contexts, model.context))
        retrieval_supports = np.vstack((retrieval_supports, model.outcome_probabilities(
            activation_cue(model))))

    # actual recall
    while model.retrieving:
        model.free_recall(1)
        retrieval_contexts = np.vstack((retrieval_contexts, model.context))
        retrieval_supports = np.vstack((retrieval_supports, model.outcome_probabilities(
            activation_cue(model))))

    return retrieval_contexts, retrieval_supports, model.recall[:model.recall_total]

# Cell

def outcome_probs_at_index(model, support_index_to_plot=1, savefig=True):
    """
    Plots outcome probability distribution at a specific index of free recall.

    **Required model attributes**:
    - item_count: specifies number of items encoded into memory
    - context: vector representing an internal contextual state
    - experience: adding a new trace to the memory model
    - activations: function returning item activations given a vector probe
    - outcome_probabilities: function returning item supports given a set of activations
    - free_recall: function that freely recalls a given number of items or until recall stops
    - state: indicates whether model is encoding or engaged in recall with a string

    **Other arguments**:
    - support_index_to_plot: index of retrieval to plot
    - savefig: whether to save or display the figure of interest

    **Generates** a plot of outcome probabilities as a line graph. Also returns vector representation of the
    generated probabilities.
    """

    retrieval_supports = retrieval_states(model)[1]
    plt.plot(np.arange(model.item_count + 1), retrieval_supports[support_index_to_plot])
    plt.xlabel('Choice Index')
    plt.ylabel('Outcome Probability')
    plt.title('Outcome Probabilities At Recall Index {}'.format(support_index_to_plot))
    plt.show()
    return retrieval_supports[support_index_to_plot]

# Cell

def retrieval_visualizations(model, savefig=True):
    """
    Plots incremental retrieval contexts and supports, as heatmaps, and prints recalled items.

    **Required model attributes**:
    - item_count: specifies number of items encoded into memory
    - context: vector representing an internal contextual state
    - experience: adding a new trace to the memory model
    - activations: function returning item activations given a vector probe
    - outcome_probabilities: function returning item supports given a set of activations

    **Also** uses savefig: boolean deciding whether figures are saved (True) or displayed
    """

    retrieval_contexts, retrieval_supports, recall = retrieval_states(model)
    plot_states(retrieval_contexts, 'Retrieval Contexts', savefig=savefig)
    plot_states(retrieval_supports, 'Supports For Each Item At Each Increment of Retrieval',
                savefig=savefig)
    return recall

# Cell

import pandas as pd
import seaborn as sns
from psifr import fr
import matplotlib.pyplot as plt

def visualize_individuals(data, data_query='subject > -1'):
    """
    Visualize variation between subjects in dataset wrt key organizational metrics.
    """

    # generate data-based spc, pnr, lag_crp
    data_spc = fr.spc(data).query(data_query).reset_index()
    data_pfr = fr.pnr(data).query('output <= 1').query(data_query).reset_index()
    data_lag_crp = fr.lag_crp(data).query(data_query).reset_index()

    # spc
    g = sns.FacetGrid(dropna=False, data=data_spc)
    g.map_dataframe(sns.lineplot, x='input', y='recall', hue='subject')
    g.set_xlabels('Serial position')
    g.set_ylabels('Recall probability')
    #plt.title('Recall Probability by Serial Position')
    g.set(ylim=(0, 1))
    plt.savefig('spc.pdf', bbox_inches='tight')

    # pfr
    h = sns.FacetGrid(dropna=False, data=data_pfr)
    h.map_dataframe(sns.lineplot, x='input', y='prob', hue='subject')
    h.set_xlabels('Serial position')
    h.set_ylabels('Probability of First Recall')
    #plt.title('P(First Recall) by Serial Position')
    h.set(ylim=(0, 1))
    plt.savefig('pfr.pdf', bbox_inches='tight')

    # lag crp
    max_lag = 5
    filt_neg = f'{-max_lag} <= lag < 0'
    filt_pos = f'0 < lag <= {max_lag}'
    i = sns.FacetGrid(dropna=False, data=data_lag_crp)
    i.map_dataframe(
        lambda data, **kws: sns.lineplot(data=data.query(filt_neg),
                                         x='lag', y='prob', hue='subject', **kws))
    i.map_dataframe(
        lambda data, **kws: sns.lineplot(data=data.query(filt_pos),
                                         x='lag', y='prob', hue='subject', **kws))
    i.set_xlabels('Item Lag')
    i.set_ylabels('Conditional Response Probability')
    #plt.title('Recall Probability by Item Lag')
    i.set(ylim=(0, 1))
    plt.savefig('crp.pdf', bbox_inches='tight')

# Cell

import pandas as pd
import seaborn as sns
from psifr import fr
import matplotlib.pyplot as plt

def visualize_aggregate(data, data_query):

    # generate data-based spc, pnr, lag_crp
    data_spc = fr.spc(data).query(data_query).reset_index()
    data_pfr = fr.pnr(data).query('output <= 1').query(data_query).reset_index()
    data_lag_crp = fr.lag_crp(data).query(data_query).reset_index()

    # spc
    g = sns.FacetGrid(dropna=False, data=data_spc)
    g.map_dataframe(sns.lineplot, x='input', y='recall',)
    g.set_xlabels('Serial position')
    g.set_ylabels('Recall probability')
    #plt.title('Recall Probability by Serial Position')
    g.set(ylim=(0, 1))
    plt.savefig('spc.pdf', bbox_inches='tight')

    # pfr
    h = sns.FacetGrid(dropna=False, data=data_pfr)
    h.map_dataframe(sns.lineplot, x='input', y='prob')
    h.set_xlabels('Serial position')
    h.set_ylabels('Probability of First Recall')
    #plt.title('P(First Recall) by Serial Position')
    h.set(ylim=(0, 1))
    plt.savefig('pfr.pdf', bbox_inches='tight')

    # lag crp
    max_lag = 5
    filt_neg = f'{-max_lag} <= lag < 0'
    filt_pos = f'0 < lag <= {max_lag}'
    i = sns.FacetGrid(dropna=False, data=data_lag_crp)
    i.map_dataframe(
        lambda data, **kws: sns.lineplot(data=data.query(filt_neg),
                                         x='lag', y='prob', **kws))
    i.map_dataframe(
        lambda data, **kws: sns.lineplot(data=data.query(filt_pos),
                                         x='lag', y='prob', **kws))
    i.set_xlabels('Item Lag')
    i.set_ylabels('Conditional Response Probability')
    #plt.title('Recall Probability by Item Lag')
    i.set(ylim=(0, 1))
    plt.savefig('crp.pdf', bbox_inches='tight')

# Cell

from .datasets import simulate_data

def visualize_model(model, experiment_count, first_recall_item=None):

    visualize_aggregate(simulate_data(model, experiment_count, first_recall_item), data_query)

# Cell

import pandas as pd
import seaborn as sns
from psifr import fr
import matplotlib.pyplot as plt

def visualize_fit(
    model_class, parameters, data, data_query=None, experiment_count=1000, savefig=False):

    """
    Apply organizational analyses to visually compare the behavior of the model
    with these parameters against specified dataset.
    """

    # generate simulation data from model
    model = model_class(**parameters)
    sim_data = simulate_data(model, experiment_count)

    # generate simulation-based spc, pnr, lag_crp
    sim_spc = fr.spc(sim_data).reset_index()
    sim_pfr = fr.pnr(sim_data).query('output <= 1') .reset_index()
    sim_lag_crp = fr.lag_crp(sim_data).reset_index()

    # generate data-based spc, pnr, lag_crp
    data_spc = fr.spc(data).query(data_query).reset_index()
    data_pfr = fr.pnr(data).query('output <= 1').query(data_query).reset_index()
    data_lag_crp = fr.lag_crp(data).query(data_query).reset_index()

    # combine representations
    data_spc['Source'] = 'Data'
    sim_spc['Source'] = model_class.__name__
    combined_spc = pd.concat([data_spc, sim_spc], axis=0)

    data_pfr['Source'] = 'Data'
    sim_pfr['Source'] = model_class.__name__
    combined_pfr = pd.concat([data_pfr, sim_pfr], axis=0)

    data_lag_crp['Source'] = 'Data'
    sim_lag_crp['Source'] = model_class.__name__
    combined_lag_crp = pd.concat([data_lag_crp, sim_lag_crp], axis=0)

    # generate plots of result
    # spc
    g = sns.FacetGrid(dropna=False, data=combined_spc)
    g.map_dataframe(sns.lineplot, x='input', y='recall', hue='Source')
    g.set_xlabels('Serial position')
    g.set_ylabels('Recall probability')
    #plt.title('Recall Probability by Serial Position')
    g.add_legend()
    g.set(ylim=(0, 1))
    plt.savefig('{}_fit_spc.pdf'.format(model_class.__name__), bbox_inches='tight')

    #pdf
    h = sns.FacetGrid(dropna=False, data=data_pfr)
    h.map_dataframe(sns.lineplot, x='input', y='prob', hue='Source')
    h.set_xlabels('Serial position')
    h.set_ylabels('Probability of First Recall')
    #plt.title('P(First Recall) by Serial Position')
    h.add_legend()
    h.set(ylim=(0, 1))
    plt.savefig('{}_fit_pfr.pdf'.format(model_class.__name__), bbox_inches='tight')

    # lag crp
    max_lag = 5
    filt_neg = f'{-max_lag} <= lag < 0'
    filt_pos = f'0 < lag <= {max_lag}'
    i = sns.FacetGrid(dropna=False, data=data_lag_crp)
    i.map_dataframe(
        lambda data, **kws: sns.lineplot(data=data.query(filt_neg),
                                         x='lag', y='prob', hue='Source', **kws))
    i.map_dataframe(
        lambda data, **kws: sns.lineplot(data=data.query(filt_pos),
                                         x='lag', y='prob', hue='Source', **kws))
    i.set_xlabels('Item Lag')
    i.set_ylabels('Conditional Response Probability')
    #plt.title('Recall Probability by Item Lag')
    i.add_legend()
    i.set(ylim=(0, 1))
    if savefig:
        plt.savefig('{}_fit_crp.pdf'.format(model_class.__name__), bbox_inches='tight')
    else:
        plt.show()

# Cell
from numba import njit
import numpy as np

@njit(fastmath=True, nogil=True)
def recall_probability_by_lag(presentations, trials, experiment_count=1):

    presented, retrieved = np.zeros(5), np.zeros(5)

    for trial_index, sequence in enumerate(presentations):

        for experiment in range(experiment_count):

            # retrieve trial information
            trial = trials[trial_index*experiment_count + experiment]

            # extract list of recalled items
            recalled = sequence[trial-1][trial != 0]

            for item in np.unique(sequence):
                locations = np.where(sequence == item)[0]

                # presented just once
                if len(locations) == 1:
                    index = 0
                else:

                    # no intervening items (massed)
                    lag = locations[1] - locations[0] - 1
                    if lag == 0:
                        index = 1

                    # 1-2 intervening items
                    elif lag <= 2:
                        index = 2

                    # 3-5
                    elif lag <= 5:
                        index = 3

                    # 6-8
                    else:
                        index = 4

                presented[index] += 1
                retrieved[index] += item in recalled

    return retrieved, presented, retrieved/presented

# Cell

from tqdm import tqdm

def sim_recall_probability_by_lag(model_class, parameters, presentations, experiment_count=1000, savefig=False):
    """
    Apply organizational analyses to visually compare the behavior of the model
    with these parameters against specified dataset.
    """

    total_presented, total_retrieved = np.zeros(5), np.zeros(5)

    # generate simulation data from model
    for experiment in tqdm(range(experiment_count)):
        sim = np.zeros(np.shape(presentations), dtype=int)
        for trial_index, presentation in enumerate(presentations):

            item_count = np.max(presentation)+1
            model = model_class(**{'item_count': item_count, **parameters})

            # simulate study events
            try:
                model.experience(np.eye(model.item_count, model.item_count + 1, 1)[presentation])
            except ValueError:
                model.experience(np.eye(model.item_count, model.item_count)[presentation])

            # simulate and add recall events to trials array
            recalled = model.free_recall() + 1
            sim[trial_index, :len(recalled)] = recalled

        retrieved, presented = recall_probability_by_lag(presentations, sim)[:2]
        total_presented += presented
        total_retrieved += retrieved

    return total_retrieved, total_presented, total_retrieved/total_presented